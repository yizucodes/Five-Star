{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cell - Essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_theme()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1689188 reviews\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "def load_data():\n",
    "    \"\"\"Load the explored dataset from previous notebook\"\"\"\n",
    "    try:\n",
    "        df = pd.read_json('../data/raw/reviews_Electronics_5.json.gz', lines=True)\n",
    "        print(f\"Loaded {len(df)} reviews\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing class\n",
    "class ReviewPreprocessor:\n",
    "    \"\"\"Class to handle all preprocessing steps for Amazon reviews\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common English stopwords - we can define these ourselves\n",
    "        self.stop_words = {\n",
    "            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "            \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "            'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
    "            'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "            'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', \n",
    "            'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', \n",
    "            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "            'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n",
    "            'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "            'with', 'about', 'against', 'between', 'into', 'through', 'during', \n",
    "            'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "            'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', \n",
    "            'then', 'once'\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean review text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove stopwords without using NLTK\"\"\"\n",
    "        # Simple word splitting (no need for complex tokenization)\n",
    "        words = text.split()\n",
    "        # Remove stopwords\n",
    "        filtered_words = [word for word in words if word not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def process_dates(self, df):\n",
    "        \"\"\"Process and format dates\"\"\"\n",
    "        df['review_date'] = pd.to_datetime(df['unixReviewTime'], unit='s')\n",
    "        df['formatted_date'] = df['review_date'].dt.strftime('%B %d, %Y')\n",
    "        return df\n",
    "    \n",
    "    def create_features(self, df):\n",
    "        \"\"\"Create additional features\"\"\"\n",
    "        df['review_length'] = df['reviewText'].str.len()\n",
    "        df['word_count'] = df['reviewText'].str.split().str.len()\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Define preprocessing function\n",
    "def preprocess_data(df, batch_size=10000):\n",
    "    \"\"\"Main preprocessing pipeline\"\"\"\n",
    "    df = df.copy()\n",
    "    preprocessor = ReviewPreprocessor()\n",
    "    print(f\"Starting preprocessing pipeline for {len(df)} reviews...\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    initial_size = len(df)\n",
    "    df = df.drop_duplicates(subset=['reviewText'])\n",
    "    print(f\"Removed {initial_size - len(df)} duplicate reviews\")\n",
    "    \n",
    "    # Process in batches\n",
    "    total_batches = len(df) // batch_size + 1\n",
    "    print(\"Cleaning text and removing stopwords...\")\n",
    "    for i in range(total_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        print(f\"Processing batch {i+1}/{total_batches}\")\n",
    "        \n",
    "        # Clean and process text\n",
    "        df.loc[start_idx:end_idx, 'cleaned_text'] = \\\n",
    "            df.loc[start_idx:end_idx, 'reviewText'].apply(preprocessor.clean_text)\n",
    "        df.loc[start_idx:end_idx, 'processed_text'] = \\\n",
    "            df.loc[start_idx:end_idx, 'cleaned_text'].apply(preprocessor.remove_stopwords)\n",
    "    \n",
    "    # Process dates and create features\n",
    "    df = preprocessor.process_dates(df)\n",
    "    df = preprocessor.create_features(df)\n",
    "    \n",
    "    print(\"Preprocessing complete!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Define verification function\n",
    "def verify_preprocessing(df):\n",
    "    \"\"\"Verify the preprocessing results\"\"\"\n",
    "    print(\"Preprocessing Verification:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\n1. Data Shape:\")\n",
    "    print(f\"Number of reviews: {len(df)}\")\n",
    "    \n",
    "    print(\"\\n2. Text Cleaning Check:\")\n",
    "    print(\"Sample original text:\")\n",
    "    print(df['reviewText'].iloc[0][:200])\n",
    "    print(\"\\nSample cleaned text:\")\n",
    "    print(df['processed_text'].iloc[0][:200])\n",
    "    \n",
    "    print(\"\\n3. Feature Statistics:\")\n",
    "    print(df[['review_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Define save function\n",
    "def save_processed_data(df, filename='../data/processed/processed_reviews.csv'):\n",
    "    \"\"\"Save the processed dataset\"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved processed data to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing on sample...\n",
      "Starting preprocessing pipeline for 1000 reviews...\n",
      "Removed 1 duplicate reviews\n",
      "Cleaning text and removing stopwords...\n",
      "Processing batch 1/1\n",
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 - Process sample data\n",
    "# Process a small sample first\n",
    "sample_df = df.head(1000)  # Start with 1000 reviews\n",
    "print(\"Testing preprocessing on sample...\")\n",
    "processed_sample = preprocess_data(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Verification:\n",
      "--------------------------------------------------\n",
      "\n",
      "1. Data Shape:\n",
      "Number of reviews: 999\n",
      "\n",
      "2. Text Cleaning Check:\n",
      "Sample original text:\n",
      "We got this GPS for my husband who is an (OTR) over the road trucker.  Very Impressed with the shipping time, it arrived a few days earlier than expected...  within a week of use however it started fr\n",
      "\n",
      "Sample cleaned text:\n",
      "got gps husband otr road trucker very impressed shipping time arrived few days earlier than expected within week use however started freezing could just glitch unit worked great when worked will work \n",
      "\n",
      "3. Feature Statistics:\n",
      "       review_length   word_count\n",
      "count     999.000000   999.000000\n",
      "mean      789.534535   146.160160\n",
      "std      1188.697647   215.970236\n",
      "min         0.000000     0.000000\n",
      "25%       165.500000    32.000000\n",
      "50%       345.000000    67.000000\n",
      "75%       871.000000   165.000000\n",
      "max     11622.000000  2079.000000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 5 - Verify sample processing\n",
    "verify_preprocessing(processed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing full dataset...\n",
      "Starting preprocessing pipeline for 1689188 reviews...\n",
      "Removed 2019 duplicate reviews\n",
      "Cleaning text and removing stopwords...\n",
      "Processing batch 1/169\n",
      "Processing batch 2/169\n",
      "Processing batch 3/169\n",
      "Processing batch 4/169\n",
      "Processing batch 5/169\n",
      "Processing batch 6/169\n",
      "Processing batch 7/169\n",
      "Processing batch 8/169\n",
      "Processing batch 9/169\n",
      "Processing batch 10/169\n",
      "Processing batch 11/169\n",
      "Processing batch 12/169\n",
      "Processing batch 13/169\n",
      "Processing batch 14/169\n",
      "Processing batch 15/169\n",
      "Processing batch 16/169\n",
      "Processing batch 17/169\n",
      "Processing batch 18/169\n",
      "Processing batch 19/169\n",
      "Processing batch 20/169\n",
      "Processing batch 21/169\n",
      "Processing batch 22/169\n",
      "Processing batch 23/169\n",
      "Processing batch 24/169\n",
      "Processing batch 25/169\n",
      "Processing batch 26/169\n",
      "Processing batch 27/169\n",
      "Processing batch 28/169\n",
      "Processing batch 29/169\n",
      "Processing batch 30/169\n",
      "Processing batch 31/169\n",
      "Processing batch 32/169\n",
      "Processing batch 33/169\n",
      "Processing batch 34/169\n",
      "Processing batch 35/169\n",
      "Processing batch 36/169\n",
      "Processing batch 37/169\n",
      "Processing batch 38/169\n",
      "Processing batch 39/169\n",
      "Processing batch 40/169\n",
      "Processing batch 41/169\n",
      "Processing batch 42/169\n",
      "Processing batch 43/169\n",
      "Processing batch 44/169\n",
      "Processing batch 45/169\n",
      "Processing batch 46/169\n",
      "Processing batch 47/169\n",
      "Processing batch 48/169\n",
      "Processing batch 49/169\n",
      "Processing batch 50/169\n",
      "Processing batch 51/169\n",
      "Processing batch 52/169\n",
      "Processing batch 53/169\n",
      "Processing batch 54/169\n",
      "Processing batch 55/169\n",
      "Processing batch 56/169\n",
      "Processing batch 57/169\n",
      "Processing batch 58/169\n",
      "Processing batch 59/169\n",
      "Processing batch 60/169\n",
      "Processing batch 61/169\n",
      "Processing batch 62/169\n",
      "Processing batch 63/169\n",
      "Processing batch 64/169\n",
      "Processing batch 65/169\n",
      "Processing batch 66/169\n",
      "Processing batch 67/169\n",
      "Processing batch 68/169\n",
      "Processing batch 69/169\n",
      "Processing batch 70/169\n",
      "Processing batch 71/169\n",
      "Processing batch 72/169\n",
      "Processing batch 73/169\n",
      "Processing batch 74/169\n",
      "Processing batch 75/169\n",
      "Processing batch 76/169\n",
      "Processing batch 77/169\n",
      "Processing batch 78/169\n",
      "Processing batch 79/169\n",
      "Processing batch 80/169\n",
      "Processing batch 81/169\n",
      "Processing batch 82/169\n",
      "Processing batch 83/169\n",
      "Processing batch 84/169\n",
      "Processing batch 85/169\n",
      "Processing batch 86/169\n",
      "Processing batch 87/169\n",
      "Processing batch 88/169\n",
      "Processing batch 89/169\n",
      "Processing batch 90/169\n",
      "Processing batch 91/169\n",
      "Processing batch 92/169\n",
      "Processing batch 93/169\n",
      "Processing batch 94/169\n",
      "Processing batch 95/169\n",
      "Processing batch 96/169\n",
      "Processing batch 97/169\n",
      "Processing batch 98/169\n",
      "Processing batch 99/169\n",
      "Processing batch 100/169\n",
      "Processing batch 101/169\n",
      "Processing batch 102/169\n",
      "Processing batch 103/169\n",
      "Processing batch 104/169\n",
      "Processing batch 105/169\n",
      "Processing batch 106/169\n",
      "Processing batch 107/169\n",
      "Processing batch 108/169\n",
      "Processing batch 109/169\n",
      "Processing batch 110/169\n",
      "Processing batch 111/169\n",
      "Processing batch 112/169\n",
      "Processing batch 113/169\n",
      "Processing batch 114/169\n",
      "Processing batch 115/169\n",
      "Processing batch 116/169\n",
      "Processing batch 117/169\n",
      "Processing batch 118/169\n",
      "Processing batch 119/169\n",
      "Processing batch 120/169\n",
      "Processing batch 121/169\n",
      "Processing batch 122/169\n",
      "Processing batch 123/169\n",
      "Processing batch 124/169\n",
      "Processing batch 125/169\n",
      "Processing batch 126/169\n",
      "Processing batch 127/169\n",
      "Processing batch 128/169\n",
      "Processing batch 129/169\n",
      "Processing batch 130/169\n",
      "Processing batch 131/169\n",
      "Processing batch 132/169\n",
      "Processing batch 133/169\n",
      "Processing batch 134/169\n",
      "Processing batch 135/169\n",
      "Processing batch 136/169\n",
      "Processing batch 137/169\n",
      "Processing batch 138/169\n",
      "Processing batch 139/169\n",
      "Processing batch 140/169\n",
      "Processing batch 141/169\n",
      "Processing batch 142/169\n",
      "Processing batch 143/169\n",
      "Processing batch 144/169\n",
      "Processing batch 145/169\n",
      "Processing batch 146/169\n",
      "Processing batch 147/169\n",
      "Processing batch 148/169\n",
      "Processing batch 149/169\n",
      "Processing batch 150/169\n",
      "Processing batch 151/169\n",
      "Processing batch 152/169\n",
      "Processing batch 153/169\n",
      "Processing batch 154/169\n",
      "Processing batch 155/169\n",
      "Processing batch 156/169\n",
      "Processing batch 157/169\n",
      "Processing batch 158/169\n",
      "Processing batch 159/169\n",
      "Processing batch 160/169\n",
      "Processing batch 161/169\n",
      "Processing batch 162/169\n",
      "Processing batch 163/169\n",
      "Processing batch 164/169\n",
      "Processing batch 165/169\n",
      "Processing batch 166/169\n",
      "Processing batch 167/169\n",
      "Processing batch 168/169\n",
      "Processing batch 169/169\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 - Process full dataset (only run if sample looks good) -- TODO: Check if need to do for whole dataset or only 5000 reviews?\n",
    "print(\"Processing full dataset...\")\n",
    "processed_df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - Verify full dataset\n",
    "verify_preprocessing(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 - Save processed data\n",
    "save_processed_data(processed_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
